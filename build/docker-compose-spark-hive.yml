version: "3.8"
services:
  
    jupyterlab:
        image: mrussorb/cluster-jupyterlab:3.0.14
        container_name: jupyterlab
        ports:
            - 8888:8888
        volumes:
            - shared-workspace:/opt/workspace
        networks:
           - sparkhivenet

    spark-hive-master:
        image: spark-hive-master:3.1.1-hive3.1.2
        container_name: spark-hive-master
        expose:
            - "7077"
            - "8080"
            - "4040"
            - "10000"
            - "10001"
            - "9083"
        ports:
            - 7077:7077
            - 8080:8080
            - 4040:4040
            - 10000:10000
            - 10001:10001
            - 9083:9083
        volumes:
            - shared-workspace:/opt/workspace
        networks: 
            - sparkhivenet
                
            
    spark-hive-worker-1:
        image: spark-hive-worker:3.1.1-hive3.1.2
        container_name: spark-hive-worker-1
        environment:
            - SPARK_WORKER_CORES=1
            - SPARK_WORKER_MEMORY=512m
        ports:
            - 8081:8081
        volumes:
            - shared-workspace:/opt/workspace
        depends_on:
            - spark-hive-master
        networks:
            - sparkhivenet

    spark-hive-worker-2:
        image: spark-hive-worker:3.1.1-hive3.1.2
        container_name: spark-hive-worker-2
        environment:
            - SPARK_WORKER_CORES=1
            - SPARK_WORKER_MEMORY=512m
        ports:
            - 8082:8081
        volumes:
            - shared-workspace:/opt/workspace
        depends_on:
            - spark-hive-master
        networks: 
            - sparkhivenet

    spark-hive-worker-3:
        image: spark-hive-worker:3.1.1-hive3.1.2
        container_name: spark-hive-worker-3
        environment:
            - SPARK_WORKER_CORES=1
            - SPARK_WORKER_MEMORY=512m
        ports:
            - 8083:8081
        volumes:
            - shared-workspace:/opt/workspace
        depends_on:
            - spark-hive-master
        networks: 
            - sparkhivenet
    
volumes:
    shared-workspace:
        name: "hadoop-distributed-file-system"
        driver: local

networks:
    sparkhivenet:
        name: sparkhivenet
        driver: bridge
