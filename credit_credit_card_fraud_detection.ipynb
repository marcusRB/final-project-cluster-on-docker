{"cells":[{"cell_type":"markdown","source":["In this project, I have implemented credit card fraud detection model using Spark and LightGBMClassifier in Databricks runtime environment using dataset provided by Machine Learning Group at Universit√© libre de Bruxelles (ULB). The dataset with 300,000 rows consisting 31 variables related to European Credit Card holder's transactions out of which 28 are numeric variables derived by performing Principal Component Analysis on some unrevealed original parameters. The remaining three variables are Amount of transaction, time of transaction in seconds relative to first tranaction and Class of transaction indicating whether its genuine or fradulent."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport pyspark\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as F\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\nfrom pyspark.ml.classification import LogisticRegression, GBTClassifier, RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml import Pipeline\nfrom mmlspark import LightGBMClassifier"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/creditcard.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["pd.DataFrame(df.take(5), columns=df.columns).transpose()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Time</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>V1</th>\n      <td>-1.35981</td>\n      <td>1.19186</td>\n      <td>-1.35835</td>\n      <td>-0.966272</td>\n      <td>-1.15823</td>\n    </tr>\n    <tr>\n      <th>V2</th>\n      <td>-0.0727812</td>\n      <td>0.266151</td>\n      <td>-1.34016</td>\n      <td>-0.185226</td>\n      <td>0.877737</td>\n    </tr>\n    <tr>\n      <th>V3</th>\n      <td>2.53635</td>\n      <td>0.16648</td>\n      <td>1.77321</td>\n      <td>1.79299</td>\n      <td>1.54872</td>\n    </tr>\n    <tr>\n      <th>V4</th>\n      <td>1.37816</td>\n      <td>0.448154</td>\n      <td>0.37978</td>\n      <td>-0.863291</td>\n      <td>0.403034</td>\n    </tr>\n    <tr>\n      <th>V5</th>\n      <td>-0.338321</td>\n      <td>0.0600176</td>\n      <td>-0.503198</td>\n      <td>-0.0103089</td>\n      <td>-0.407193</td>\n    </tr>\n    <tr>\n      <th>V6</th>\n      <td>0.462388</td>\n      <td>-0.0823608</td>\n      <td>1.8005</td>\n      <td>1.2472</td>\n      <td>0.0959215</td>\n    </tr>\n    <tr>\n      <th>V7</th>\n      <td>0.239599</td>\n      <td>-0.078803</td>\n      <td>0.791461</td>\n      <td>0.237609</td>\n      <td>0.592941</td>\n    </tr>\n    <tr>\n      <th>V8</th>\n      <td>0.0986979</td>\n      <td>0.0851017</td>\n      <td>0.247676</td>\n      <td>0.377436</td>\n      <td>-0.270533</td>\n    </tr>\n    <tr>\n      <th>V9</th>\n      <td>0.363787</td>\n      <td>-0.255425</td>\n      <td>-1.51465</td>\n      <td>-1.38702</td>\n      <td>0.817739</td>\n    </tr>\n    <tr>\n      <th>V10</th>\n      <td>0.0907942</td>\n      <td>-0.166974</td>\n      <td>0.207643</td>\n      <td>-0.0549519</td>\n      <td>0.753074</td>\n    </tr>\n    <tr>\n      <th>V11</th>\n      <td>-0.5516</td>\n      <td>1.61273</td>\n      <td>0.624501</td>\n      <td>-0.226487</td>\n      <td>-0.822843</td>\n    </tr>\n    <tr>\n      <th>V12</th>\n      <td>-0.617801</td>\n      <td>1.06524</td>\n      <td>0.0660837</td>\n      <td>0.178228</td>\n      <td>0.538196</td>\n    </tr>\n    <tr>\n      <th>V13</th>\n      <td>-0.99139</td>\n      <td>0.489095</td>\n      <td>0.717293</td>\n      <td>0.507757</td>\n      <td>1.34585</td>\n    </tr>\n    <tr>\n      <th>V14</th>\n      <td>-0.311169</td>\n      <td>-0.143772</td>\n      <td>-0.165946</td>\n      <td>-0.287924</td>\n      <td>-1.11967</td>\n    </tr>\n    <tr>\n      <th>V15</th>\n      <td>1.46818</td>\n      <td>0.635558</td>\n      <td>2.34586</td>\n      <td>-0.631418</td>\n      <td>0.175121</td>\n    </tr>\n    <tr>\n      <th>V16</th>\n      <td>-0.470401</td>\n      <td>0.463917</td>\n      <td>-2.89008</td>\n      <td>-1.05965</td>\n      <td>-0.451449</td>\n    </tr>\n    <tr>\n      <th>V17</th>\n      <td>0.207971</td>\n      <td>-0.114805</td>\n      <td>1.10997</td>\n      <td>-0.684093</td>\n      <td>-0.237033</td>\n    </tr>\n    <tr>\n      <th>V18</th>\n      <td>0.0257906</td>\n      <td>-0.183361</td>\n      <td>-0.121359</td>\n      <td>1.96578</td>\n      <td>-0.0381948</td>\n    </tr>\n    <tr>\n      <th>V19</th>\n      <td>0.403993</td>\n      <td>-0.145783</td>\n      <td>-2.26186</td>\n      <td>-1.23262</td>\n      <td>0.803487</td>\n    </tr>\n    <tr>\n      <th>V20</th>\n      <td>0.251412</td>\n      <td>-0.0690831</td>\n      <td>0.52498</td>\n      <td>-0.208038</td>\n      <td>0.408542</td>\n    </tr>\n    <tr>\n      <th>V21</th>\n      <td>-0.0183068</td>\n      <td>-0.225775</td>\n      <td>0.247998</td>\n      <td>-0.1083</td>\n      <td>-0.0094307</td>\n    </tr>\n    <tr>\n      <th>V22</th>\n      <td>0.277838</td>\n      <td>-0.638672</td>\n      <td>0.771679</td>\n      <td>0.0052736</td>\n      <td>0.798278</td>\n    </tr>\n    <tr>\n      <th>V23</th>\n      <td>-0.110474</td>\n      <td>0.101288</td>\n      <td>0.909412</td>\n      <td>-0.190321</td>\n      <td>-0.137458</td>\n    </tr>\n    <tr>\n      <th>V24</th>\n      <td>0.0669281</td>\n      <td>-0.339846</td>\n      <td>-0.689281</td>\n      <td>-1.17558</td>\n      <td>0.141267</td>\n    </tr>\n    <tr>\n      <th>V25</th>\n      <td>0.128539</td>\n      <td>0.16717</td>\n      <td>-0.327642</td>\n      <td>0.647376</td>\n      <td>-0.20601</td>\n    </tr>\n    <tr>\n      <th>V26</th>\n      <td>-0.189115</td>\n      <td>0.125895</td>\n      <td>-0.139097</td>\n      <td>-0.221929</td>\n      <td>0.502292</td>\n    </tr>\n    <tr>\n      <th>V27</th>\n      <td>0.133558</td>\n      <td>-0.0089831</td>\n      <td>-0.0553528</td>\n      <td>0.0627228</td>\n      <td>0.219422</td>\n    </tr>\n    <tr>\n      <th>V28</th>\n      <td>-0.0210531</td>\n      <td>0.0147242</td>\n      <td>-0.0597518</td>\n      <td>0.0614576</td>\n      <td>0.215153</td>\n    </tr>\n    <tr>\n      <th>Amount</th>\n      <td>149.62</td>\n      <td>2.69</td>\n      <td>378.66</td>\n      <td>123.5</td>\n      <td>69.99</td>\n    </tr>\n    <tr>\n      <th>Class</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Time: decimal(10,0) (nullable = true)\n-- V1: double (nullable = true)\n-- V2: double (nullable = true)\n-- V3: double (nullable = true)\n-- V4: double (nullable = true)\n-- V5: double (nullable = true)\n-- V6: double (nullable = true)\n-- V7: double (nullable = true)\n-- V8: double (nullable = true)\n-- V9: double (nullable = true)\n-- V10: double (nullable = true)\n-- V11: double (nullable = true)\n-- V12: double (nullable = true)\n-- V13: double (nullable = true)\n-- V14: double (nullable = true)\n-- V15: double (nullable = true)\n-- V16: double (nullable = true)\n-- V17: double (nullable = true)\n-- V18: double (nullable = true)\n-- V19: double (nullable = true)\n-- V20: double (nullable = true)\n-- V21: double (nullable = true)\n-- V22: double (nullable = true)\n-- V23: double (nullable = true)\n-- V24: double (nullable = true)\n-- V25: double (nullable = true)\n-- V26: double (nullable = true)\n-- V27: double (nullable = true)\n-- V28: double (nullable = true)\n-- Amount: double (nullable = true)\n-- Class: integer (nullable = true)\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["df.groupBy(\"Class\").count().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+------+\nClass| count|\n+-----+------+\n    1|   492|\n    0|284315|\n+-----+------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["As seen in the above counts for each class value, the dataset is heavily imbalanced. One approach that we can use is to assign different weights to classes in our binary classifier."],"metadata":{}},{"cell_type":"code","source":["feature_cols = [\"V\" + str(i) for i in range(1,29)] + [\"Amount\"]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["stages = [assembler]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["I derived below params while training LightGBM model for this dataset in Python enviornment using Tree of Parzen Estimators algorithm implemented in Hyperopt library. Using them right now to get started quickly with great performance on our test outcome. It's also possible to do model tuning inside Spark using ParamGridBuilder and CrossValidator, although it will take more time to explore the hyperparameter space without using Bayesian Optimization or Tree of Parzen's Estimator algorithm to quickly find optimal parameters."],"metadata":{}},{"cell_type":"markdown","source":["https://www.kaggle.com/patelatharva/credit-card-transaction-fraud-detection"],"metadata":{}},{"cell_type":"code","source":["best_params = {   \n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'eval_metric': 'binary_error',\n    'feature_fraction': 0.944714847210862,\n    'lambda_l1': 1.0,\n    'lambda_l2': 45.0,\n    'learning_rate': 0.1,\n    'loss_function': 'binary_error',\n    'max_bin': 60,\n    'max_depth': 58,\n    'metric': 'binary_error',\n    'num_iterations': 379,\n    'num_leaves': 850,\n    'objective': 'binary',\n    'random_state': 7,\n    'verbose': None}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["lgb = LightGBMClassifier(learningRate=0.1,\n                  earlyStoppingRound=100,\n                  featuresCol='features',\n                  labelCol='Class',\n                  isUnbalance=True,\n                  baggingFraction=best_params[\"bagging_fraction\"],\n                  baggingFreq=1,\n                  featureFraction=best_params[\"feature_fraction\"],\n                  lambdaL1=best_params[\"lambda_l1\"],\n                  lambdaL2=best_params[\"lambda_l2\"],\n                  maxBin=best_params[\"max_bin\"],\n                  maxDepth=best_params[\"max_depth\"],\n                  numIterations=best_params[\"num_iterations\"],\n                  numLeaves=best_params[\"num_leaves\"],\n                  objective=\"binary\",\n                  baggingSeed=7\n                  )\nstages += [lgb]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["pipelineModel = Pipeline(stages=stages)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Time: decimal(10,0) (nullable = true)\n-- V1: double (nullable = true)\n-- V2: double (nullable = true)\n-- V3: double (nullable = true)\n-- V4: double (nullable = true)\n-- V5: double (nullable = true)\n-- V6: double (nullable = true)\n-- V7: double (nullable = true)\n-- V8: double (nullable = true)\n-- V9: double (nullable = true)\n-- V10: double (nullable = true)\n-- V11: double (nullable = true)\n-- V12: double (nullable = true)\n-- V13: double (nullable = true)\n-- V14: double (nullable = true)\n-- V15: double (nullable = true)\n-- V16: double (nullable = true)\n-- V17: double (nullable = true)\n-- V18: double (nullable = true)\n-- V19: double (nullable = true)\n-- V20: double (nullable = true)\n-- V21: double (nullable = true)\n-- V22: double (nullable = true)\n-- V23: double (nullable = true)\n-- V24: double (nullable = true)\n-- V25: double (nullable = true)\n-- V26: double (nullable = true)\n-- V27: double (nullable = true)\n-- V28: double (nullable = true)\n-- Amount: double (nullable = true)\n-- Class: integer (nullable = true)\n\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Split data into training and test datasets."],"metadata":{}},{"cell_type":"code","source":["train, test = df.randomSplit([0.8, 0.2], seed=7)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["train.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[107]: 227900</div>"]}}],"execution_count":19},{"cell_type":"code","source":["test.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[108]: 56907</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["Fitting the model using training data."],"metadata":{}},{"cell_type":"code","source":["model = pipelineModel.fit(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["Making predictions on test data."],"metadata":{}},{"cell_type":"code","source":["preds = model.transform(test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["preds.select('Class', 'prediction', 'probability').show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+--------------------+\nClass|prediction|         probability|\n+-----+----------+--------------------+\n    0|       0.0|[0.99999975096536...|\n    0|       0.0|[0.99999964004611...|\n    0|       0.0|[0.97189974821673...|\n    0|       0.0|[0.99999936600427...|\n    0|       0.0|[0.99999996266190...|\n    0|       0.0|[0.99930486046750...|\n    0|       0.0|[0.99999974633342...|\n    0|       0.0|[0.99989036386859...|\n    0|       0.0|[0.99999999999995...|\n    0|       0.0|[0.99999999999995...|\n+-----+----------+--------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["Evaluating predictions"],"metadata":{}},{"cell_type":"code","source":["binaryEvaluator = BinaryClassificationEvaluator(labelCol=\"Class\")\nprint (\"Test Area Under ROC: \" + str(binaryEvaluator.evaluate(preds, {binaryEvaluator.metricName: \"areaUnderROC\"})))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Test Area Under ROC: 0.8955919603138618\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["tp = preds[(preds.Class == 1) & (preds.prediction == 1)].count()\ntn = preds[(preds.Class == 0) & (preds.prediction == 0)].count()\nfp = preds[(preds.Class == 0) & (preds.prediction == 1)].count()\nfn = preds[(preds.Class == 1) & (preds.prediction == 0)].count()\n\nprint (\"True Positives:\", tp)\n\nprint (\"True Negatives:\", tn)\n\nprint (\"False Positives:\", fp)\n\nprint (\"False Negatives:\", fn)\n\nprint (\"Total\", preds.count())\n\nr = float(tp)/(tp + fn)\n\nprint (\"recall\", r)\n\np = float(tp) / (tp + fp)\n\nprint (\"precision\", p)\n\nf1 = 2 * p * r /(p + r)\n\nprint (\"f1\", f1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">True Positives: 75\nTrue Negatives: 56771\nFalse Positives: 38\nFalse Negatives: 23\nTotal 56907\nrecall 0.7653061224489796\nprecision 0.6637168141592921\nf1 0.7109004739336493\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["Directions to improve on F1 score and AUC ROC:\n1. Implementing Synthentic Minority Oversampling in Spark here before using the imbalanced training data for fitting the model. Currently I have relied upon LightGBMClassifier's isUnbalance=True flag to take care of this imbalance.\n2. Exploring the hyperparameter space more deeply to find out optimal values. The hyperparameters values that I have used here were found by doing 200 evaluations of different combinations of Parameter values using Hyperopt library with TPE algorithm for exploring hyperparam space. Here is my notebook where I first found out best parameter values to be used in this project https://www.kaggle.com/patelatharva/credit-card-transaction-fraud-detection"],"metadata":{}}],"metadata":{"name":"credit card fraud detection","notebookId":1558268608003263},"nbformat":4,"nbformat_minor":0}
